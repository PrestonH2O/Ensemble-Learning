{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier\n",
    "Using data from: https://www.kaggle.com/datasets/luvharishkhati/heart-disease-patients-details/data\n",
    "\n",
    "Code help from: https://www.datacamp.com/tutorial/decision-tree-classification-python\n",
    "\n",
    "Article help from: https://www.techtarget.com/searchenterpriseai/definition/data-splitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result\n",
      "0    150\n",
      "1    120\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>chest</th>\n",
       "      <th>resting_blood_pressure</th>\n",
       "      <th>serum_cholestoral</th>\n",
       "      <th>fasting_blood_sugar</th>\n",
       "      <th>resting_electrocardiographic_results</th>\n",
       "      <th>maximum_heart_rate_achieved</th>\n",
       "      <th>exercise_induced_angina</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>number_of_major_vessels</th>\n",
       "      <th>thal</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>130</td>\n",
       "      <td>322</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>115</td>\n",
       "      <td>564</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>160</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>124</td>\n",
       "      <td>261</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>128</td>\n",
       "      <td>263</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>120</td>\n",
       "      <td>269</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  chest  resting_blood_pressure  serum_cholestoral  \\\n",
       "0   70    1      4                     130                322   \n",
       "1   67    0      3                     115                564   \n",
       "2   57    1      2                     124                261   \n",
       "3   64    1      4                     128                263   \n",
       "4   74    0      2                     120                269   \n",
       "\n",
       "   fasting_blood_sugar  resting_electrocardiographic_results  \\\n",
       "0                    0                                     2   \n",
       "1                    0                                     2   \n",
       "2                    0                                     0   \n",
       "3                    0                                     0   \n",
       "4                    0                                     2   \n",
       "\n",
       "   maximum_heart_rate_achieved  exercise_induced_angina  oldpeak  slope  \\\n",
       "0                          109                        0      2.4      2   \n",
       "1                          160                        0      1.6      2   \n",
       "2                          141                        0      0.3      1   \n",
       "3                          105                        1      0.2      2   \n",
       "4                          121                        1      0.2      1   \n",
       "\n",
       "   number_of_major_vessels  thal  result  \n",
       "0                        3     3       1  \n",
       "1                        0     7       0  \n",
       "2                        0     7       1  \n",
       "3                        1     7       0  \n",
       "4                        1     3       0  "
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Data source: https://www.kaggle.com/datasets/luvharishkhati/heart-disease-patients-details/data\n",
    "df = pd.read_csv('heart_disease.csv')\n",
    "\n",
    "# X is the input data in the shape (m x n) aka (examples x features)\n",
    "X = df.drop(columns=['result']).values\n",
    "\n",
    "# y is true/target value (this can only be 0 or 1) \n",
    "y = df['result'].values\n",
    "\n",
    "# Print the number of each result\n",
    "result_counts = df['result'].value_counts()\n",
    "print(result_counts)\n",
    "\n",
    "df.head() # show the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.9%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accSum = 0\n",
    "\n",
    "for i in range(1000):\n",
    "\n",
    "    # Split dataset into training set and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test\n",
    "\n",
    "    # Create Decision Tree classifer object\n",
    "    clf = DecisionTreeClassifier()\n",
    "\n",
    "    # Train Decision Tree Classifer\n",
    "    clf = clf.fit(X_train,y_train)\n",
    "\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Running total of model accuracy scores to be averaged\n",
    "    # (How often is the classifier correct?)\n",
    "    accSum += metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "    \n",
    "# Average model accuracy over 1000 runs\n",
    "print(f\"Accuracy: {(accSum/10):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The portion of the data chosen for training the model vs testing the model will effect *measured* model accuracy. The more representative the 70% of the data chosen for training is of the entire data set, the more accurate the model will appear. However, you wouldn't just pick the 70% that gives you the highest score because while it increases your model accuracy score, thats somewhat deceptive and doesn't practically increase your model quality. You're falling into the trap of overfitting and your model may generalize poorly to any new data.\n",
    "\n",
    "The above set up presents a running average over 1000 tests, this gives a reasonably accurate measurement of how good this random selection is on average, about 74%.\n",
    "\n",
    "In a real world case, you would have a large enough data set that the portion removed for model testing would hopefully effect the percent accuracy to a lower degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing feature weights\n",
    "\n",
    "Okay, so what can we mess with to increase the accuracy of the model that isn't just cherrypicking the data? How about modifying the settings on the decsion tree? Currently we're just using the default that can be found on the manual page here: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "For our cases I am going to play with \"criterion\" which selects the function used to measure the quality of a split and \"max_features\" which determines the number of features to consider when looking for the best split.\n",
    "\n",
    "Criterion:\n",
    "    gini impurity: (This is the default selection) Measures the probability of misclassifying a randomly chosen element in the set\n",
    "\n",
    "    entropy: Measures the ammount of uncertainty or randomness in a set\n",
    "\n",
    "Max features: The number of features to consider when looking for the best split\n",
    "\n",
    "    None: no selection, max_features=n_features (This is the default selection)\n",
    "\n",
    "    sqrt: max_features=sqrt(n_features)\n",
    "\n",
    "    log: max_features=log2(n_features)\n",
    "\n",
    "    int/float: input some integer or float value to cap features considered.\n",
    "\n",
    "    (There are more but these are the choices for my testing)\n",
    "\n",
    "\n",
    "Since 270 examples isn't a huge dataset, I avoided modifying features that risked overfitting since I didn't want to get a model that was \"dishonestly\" accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.4%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accSum = 0\n",
    "\n",
    "for i in range(1000):\n",
    "\n",
    "    # Split dataset into training set and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test\n",
    "\n",
    "    # Create Decision Tree classifer object\n",
    "    clf = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "\n",
    "    # Train Decision Tree Classifer\n",
    "    clf = clf.fit(X_train,y_train)\n",
    "\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Running total of model accuracy scores to be averaged\n",
    "    # (How often is the classifier correct?)\n",
    "    accSum += metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "    \n",
    "# Average model accuracy over 1000 runs\n",
    "print(f\"Accuracy: {(accSum/10):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The max_features parameter controls the maximum number of features each individual decision tree in the forest is allowed to consider when making a split.\n",
    "\n",
    "max_features=\"log2\" alone decreased accuracy to about 73% \n",
    "\n",
    "In fact, any value tested other than none, reduced the accuracy of the model. The only exception to this was setting an integer value high enough to make the number of features considered the same, since you're considering the same number of features (reconsidering them if the value is high enough). \n",
    "\n",
    "With a small dataset, say only 270 examples, None is the best choice to capture a broader range of patterns. Reducing the number of features with reducing max_features can be a regularization technique to prevent the model from becoming too complex and address overfitting.\n",
    "\n",
    "\n",
    "critereon=\"entropy\" alone increased accuracy to about 74.5%\n",
    "\n",
    "This small improvement is not surprising as ususally the difference in performance is minimal. There are situations where using entropy might lead to a slightly higher model accuracy however. Namely entropy is more sensitive to changes in class probabilities than Gini impurity. For datasets with imbalanced class distributions (cases where the outcome is biased towards one option) or scenarios where subtle differences in class probabilities matter, entropy might be better at capturing these nuances. Since the breakup of results is abotu 55% to 45% there shouldn't be much if any imbalance in the class distribution, so the slight increase in performance of entropy is likely due to subtle differences in class probabilities.\n",
    "\n",
    "\n",
    "Most of the non default options provided by DecisionTreeClassifier exist to specifically control kinds of overfitting, so changing them is going to reduce model accuracy. The other features allow better fitting but these are generally better for larger data sets. As mentioned earlier these were avoided so as not to present \"dishonest\" model accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy across 10-fold cross-validation: 82.6%\n",
      "Number of False Negatives: 32 / 270\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Create a bagging random forest classifier\n",
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "# Note: n_estimators is the number of decision trees to be used in the ensemble\n",
    "\n",
    "# Train the random forest classifier\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "k = 10  # Number of folds\n",
    "cv_scores = cross_val_score(random_forest, X, y, cv=k, scoring='accuracy')\n",
    "print(f'Mean accuracy across {k}-fold cross-validation: {(np.mean(cv_scores)*100):.1f}%')\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "k = 10  # Number of folds\n",
    "y_pred = cross_val_predict(clf, X, y, cv=k)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y, y_pred)\n",
    "\n",
    "# Extract the number of False Negatives\n",
    "false_negatives = conf_matrix[1, 0]\n",
    "\n",
    "print(f'Number of False Negatives: {false_negatives} / {sum(sum(conf_matrix))}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting: Gradient Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy across 10-fold cross-validation: 80.0%\n",
      "Number of False Negatives: 35 / 270\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Initialize the Gradient Boosting Classifier\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "\n",
    "# Fit the model on the training data\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "k = 10  # Number of folds\n",
    "cv_scores = cross_val_score(gb_classifier, X, y, cv=k, scoring='accuracy')\n",
    "print(f'Mean accuracy across {k}-fold cross-validation: {(np.mean(cv_scores)*100):.1f}%')\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "k = 10  # Number of folds\n",
    "y_pred = cross_val_predict(clf, X, y, cv=k)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y, y_pred)\n",
    "\n",
    "# Extract the number of False Negatives\n",
    "false_negatives = conf_matrix[1, 0]\n",
    "\n",
    "print(f'Number of False Negatives: {false_negatives} / {sum(sum(conf_matrix))}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and Boosting Results\n",
    "\n",
    "The random forest bagging algorithim had an accuracy of 81.5% in my testing while the gradient tree boosting algorithim had an accuracy of 80.0%. Random forest bagging was therefore a little more effective. Why was this? Let's discuss in our results section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare and Contrast Models\n",
    "\n",
    "### Metric for Evaluation\n",
    "\n",
    "Firstly, you may have noticed in the begining how clunky (and slow!) it was to analyze our decision tree classifier by just running it 1000 times. A single run wasn't a good approximation since the limited data set meant the models accuracy was hugely dependent on the random selection of training data. The score of one run wasn't a good generalization of overall model performance and 1000 runs averagef was really slow and clunky.\n",
    "\n",
    "To fix this, we use K-fold cross-validation to measure the next models. The data is divided into 'k' equally-sized folds, and trained and tested 'k' times, each time using a different fold as the test set and the remaining folds as the training set. The performance metrics from each iteration are then averaged to provide a more accurate evaluation of the model. This helps ensure that the model's performance is not heavily influenced by the specific random split of the data. K-fold cross-validation provides a more reliable estimate of how well a model generalizes to unseen data, making it a valuable tool in model evaluation and selection.\n",
    "\n",
    "Essentially, K-fold cross-validation is great for getting a more accurate estimate of the accuracy of a model's thats been fed limited data. It avoids bias, overfitting and underfitting by allowing a more representative data selection.\n",
    "\n",
    "Really quick, let's re-run decision tree classifier with k-fold analysis to better contrast results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy across 10-fold cross-validation: 74.1%\n",
      "Number of False Negatives: 34 / 270\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test\n",
    "\n",
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train,y_train)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "k = 10  # Number of folds\n",
    "cv_scores = cross_val_score(clf, X, y, cv=k, scoring='accuracy')\n",
    "print(f'Mean accuracy across {k}-fold cross-validation: {(np.mean(cv_scores)*100):.1f}%')\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "k = 10  # Number of folds\n",
    "y_pred = cross_val_predict(clf, X, y, cv=k)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y, y_pred)\n",
    "\n",
    "# Extract the number of False Negatives\n",
    "false_negatives = conf_matrix[1, 0]\n",
    "\n",
    "print(f'Number of False Negatives: {false_negatives} / {sum(sum(conf_matrix))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now that we have all three algorithms evaluated by k-fold cross validation what are our final rankings?\n",
    "(Rankings for when I ran the code, results may vary.)\n",
    "\n",
    "82.0% Random Forest Bagging\n",
    "\n",
    "80.0% Gradient Tree Boosting\n",
    "\n",
    "74.1% Decision Tree Classifier\n",
    "\n",
    "Random Forest is an ensemble learning method that builds multiple decision trees during training and merges their predictions to improve accuracy and control overfitting. Bagging is used to train each tree on a random subset of the training data. Notably, random forests perform well on high-dimensional data, with 13 features + result and 270 examples our data isn't exactly high-dimensional, it isn't super low dimensional either. Since random forests are also less prone to overfitting, the 70:30 ratio of train to test is less likely to make less accurate models.\n",
    "\n",
    "Gradient Tree Boosting is also an ensemble method. It builds a series of decision trees sequentially, with each tree correcting the errors of the previous ones. It optimizes a loss function by adding weak learners (trees) in an iterative manner. Gradient Boosting is powerful for capturing complex relationships in data and relationships with subtle patterns. This accurately describes our data so it is unsurpising that it did somewhat well.\n",
    "\n",
    "Decision trees are non-linear models that recursively split data based on feature values to make predictions.Decision tree classifier, in its basic form, can suffer from overfitting, as it tends to create deep and complex trees. This tendency to overfit tracks for this being the worse performing of our three models. The model sometimes overfits to the data selected to train and then performs poorly on the tests. Random Forest and Gradiant Tree both represent ways to help manage this issue and as such it is no surprise they both performed better.\n",
    "\n",
    "In summary,\n",
    "\n",
    "Random forest bagging and Gradient Tree Boosting perform better than the basic Decision Tree Classifier, which is consistent with expectations.\n",
    "\n",
    "Random Forest Bagging's higher accuracy can be attributed to the diversity introduced by training multiple trees on different subsets of the data, reducing overfitting.\n",
    "\n",
    "Gradient Tree Boosting's slightly lower accuracy may be due to the iterative nature of the algorithm, which can be sensitive to hyperparameter tuning. However, its effectiveness in capturing complex patterns is reflected in the performance.\n",
    "\n",
    "Decision Tree Classifier has the lowest accuracy, likely because it tends to overfit the training data by creating deep trees.\n",
    "\n",
    "\n",
    "### What about another way to evaluate?\n",
    "\n",
    "It's important to note that accuracy (which we used both in the 1000 runs and in K-fold) is just one metric, and the choice of a different metric could lead to different rankings, especially in scenarios where class imbalances exist or where certain types of errors are more important to avoid for others. For example, in medical models like this one, minimizing the number of false negatives is crucial. It would be terrible for a patient to go home without treatment thinking they're perfectly healthy! Recall Score is great for this since it judges false negatives far more harshly.\n",
    "\n",
    "Since our distribution of positive and negative results is about even, 55% to 45% and since the number of false negatives (thats why I have been calculating them) is somewhat low I don't expect our results to change much.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier mean recall score across 10-fold cross-validation: 69.2%\n",
      "Gradient Tree Boosting mean recall score across 10-fold cross-validation: 76.7%\n",
      "Random Forest Bagging mean recall score across 10-fold cross-validation: 74.2%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test\n",
    "\n",
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train,y_train)\n",
    "\n",
    "k = 10  # Number of folds\n",
    "cv_scores = cross_val_score(clf, X, y, cv=k, scoring='recall')\n",
    "print(f'Decision Tree Classifier mean recall score across {k}-fold cross-validation: {(np.mean(cv_scores)*100):.1f}%')\n",
    "\n",
    "# Initialize the Gradient Boosting Classifier\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "\n",
    "# Fit the model on the training data\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "\n",
    "k = 10  # Number of folds\n",
    "cv_scores = cross_val_score(gb_classifier, X, y, cv=k, scoring='recall')\n",
    "print(f'Gradient Tree Boosting mean recall score across {k}-fold cross-validation: {(np.mean(cv_scores)*100):.1f}%')\n",
    "\n",
    "# Create a bagging random forest classifier\n",
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Train the random forest classifier\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "k = 10  # Number of folds\n",
    "cv_scores = cross_val_score(random_forest, X, y, cv=k, scoring='recall')\n",
    "print(f'Random Forest Bagging mean recall score across {k}-fold cross-validation: {(np.mean(cv_scores)*100):.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "75.8% Random Forest Bagging\n",
    "\n",
    "76.7% Gradient Tree Boosting\n",
    "\n",
    "72.3% Decision Tree Classifier\n",
    "\n",
    "Okay so, our scores went down. This isn't that surprising, the 35 or so false negatives from each model were bound to drag the scores down. What is interesting is that Gradient Tree is now outperforming Random Forest. As far as I can tell (which isn't a very scientific measurement) there doesn't seem to be a significant difference in the number of false negatives. The outperformance likely stems from Gradient Tree's focus on correcting errors in each iteration, since one type of error is now punished more harshly, it can focus on avoiding that and therefore earns a higher score.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
